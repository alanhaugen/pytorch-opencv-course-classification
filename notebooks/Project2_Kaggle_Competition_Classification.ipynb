{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
    "\n",
    "In this section, you have to write a class or methods that will be used to get training and validation data loader.\n",
    "\n",
    "For example,\n",
    "\n",
    "```\n",
    "def get_data(args1, *agrs):\n",
    "    ....\n",
    "    ....\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # one of the best graphics libraries for python\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classesArray = [] # hack\n",
    "\n",
    "class FoodDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This custom dataset class take root directory and train flag, \n",
    "    and return dataset training dataset id train flag is true \n",
    "    else is return validation dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, train=True, image_shape=None, transform=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        init method of the class.\n",
    "        \n",
    "         Parameters:\n",
    "         \n",
    "         data_root (string): path of root directory.\n",
    "         \n",
    "         train (boolean): True for training dataset and False for test dataset.\n",
    "         \n",
    "         image_shape (int or tuple or list): [optional] int or tuple or list. Defaut is None. \n",
    "                                             It is not None image will resize to the given shape.\n",
    "                                 \n",
    "         transform (method): method that will take PIL image and transforms it.\n",
    "         \n",
    "        \"\"\"\n",
    "        \n",
    "        # get label to class mapping\n",
    "        if train:\n",
    "            label_csv_path = os.path.join(data_root, 'train.csv')\n",
    "        else:\n",
    "            label_csv_path = os.path.join(data_root, 'sample_submission.csv') # sample_submission.csv? test.csv does not have classes...\n",
    "        \n",
    "        img_dir = os.path.join(data_root, 'images', 'images')\n",
    "        \n",
    "        self.label_df = pd.read_csv(label_csv_path, delimiter=' *, *', engine='python')\n",
    "        \n",
    "        # set image_resize attribute\n",
    "        if image_shape is not None:\n",
    "            if isinstance(image_shape, int):\n",
    "                self.image_shape = (image_shape, image_shape)\n",
    "            \n",
    "            elif isinstance(image_shape, tuple) or isinstance(image_shape, list):\n",
    "                assert len(image_shape) == 1 or len(image_shape) == 2, 'Invalid image_shape tuple size'\n",
    "                if len(image_shape) == 1:\n",
    "                    self.image_shape = (image_shape[0], image_shape[0])\n",
    "                else:\n",
    "                    self.image_shape = image_shape\n",
    "            else:\n",
    "                raise NotImplementedError \n",
    "        \n",
    "        # set transform attribute\n",
    "        self.transform = transform\n",
    "        \n",
    "        # initialize the data dictionary\n",
    "        self.data_dict = {\n",
    "            'image_path': [],\n",
    "            'label': []\n",
    "        }\n",
    "        \n",
    "        # Dirty. Maybe train_test_split from sklearn would be a better option\n",
    "        for i, table in self.label_df.iterrows():\n",
    "            img = table['id']\n",
    "            img_path  = os.path.join(img_dir, str(img) + '.jpg')\n",
    "            self.data_dict['image_path'].append(img_path)\n",
    "            if train == True:\n",
    "                className = table['class']\n",
    "                if className not in classesArray:\n",
    "                    classesArray.append(className)\n",
    "                classNumber = classesArray.index(className)\n",
    "                self.data_dict['label'].append(classNumber) \n",
    "        \n",
    "        if train == False:\n",
    "            self.data_dict['label'] = None\n",
    "        else:\n",
    "            print (*self.data_dict['label'])\n",
    "        print (*self.data_dict['image_path'])\n",
    "        print (*classesArray)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        return length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data_dict['image_path'])\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        For given index, return images with resize and preprocessing.\n",
    "        \"\"\"\n",
    "        \n",
    "        image = Image.open(self.data_dict['image_path'][idx])\n",
    "        \n",
    "        if self.image_shape is not None:\n",
    "            image = image.resize(self.image_shape)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        target = self.data_dict['label'][idx]\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def get_class(self, label):\n",
    "        \"\"\"\n",
    "        class label to latin name mapping\n",
    "        \"\"\"\n",
    "        return classesArray[label] # a bit weird maybe\n",
    "\n",
    "def get_data(batch_size, data_root, tb_writer=None, num_workers=4, data_augmentation=False):\n",
    "    mean, std = get_mean_std(data_root=data_root, num_workers=num_workers)\n",
    "    \n",
    "    common_transforms = image_common_transforms(mean, std)\n",
    "   \n",
    "    # if data_augmentation is true \n",
    "    # data augmentation implementation\n",
    "    if data_augmentation:    \n",
    "        train_transforms = data_augmentation_preprocess(mean, std)\n",
    "    # else do common transforms\n",
    "    else:\n",
    "        train_transforms = common_transforms\n",
    "    \n",
    "       \n",
    "    # train dataloader\n",
    "    \n",
    "    trainDataset = FoodDataset(data_root, train=True, image_shape=256, transform=image_preprocess_transforms())\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(trainDataset, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=True,\n",
    "                               num_workers=num_workers)\n",
    "    \n",
    "    # test dataloader\n",
    "    \n",
    "    testDataset = FoodDataset(data_root, train=False, image_shape=256, transform=image_preprocess_transforms())\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(testDataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True, \n",
    "                              num_workers=num_workers)\n",
    "    \n",
    "    # Plot few images\n",
    "    plt.rcParams[\"figure.figsize\"] = (15, 9)\n",
    "    plt.figure\n",
    "    for images, labels in test_loader:\n",
    "        for i in range(10):\n",
    "            plt.subplot(3, 5, i+1)\n",
    "            img = transforms.functional.to_pil_image(images[i])\n",
    "            plt.imshow(img)\n",
    "            plt.gca().set_title('Target: {0}'.format(labels[i]))\n",
    "        plt.show()\n",
    "        break\n",
    "    \n",
    "    if tb_writer is not None:\n",
    "        add_data_embeddings(testDataset, tb_writer, n=100)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
    "\n",
    "Define your configuration in this section.\n",
    "\n",
    "For example,\n",
    "\n",
    "```\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 10 \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"./cat-dog-panda\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda'  \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 32  # amount of data to pass through the network at each forward-backward iteration\n",
    "    epochs_count: int = 4  # number of times the whole dataset will be passed through the network\n",
    "    init_learning_rate: float = 0.0001  # determines the speed of network's weights update\n",
    "    log_interval: int = 100  # how many batches to wait between logging training status\n",
    "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
    "    data_root: str = \"../data\" \n",
    "    num_workers: int = 10  # number of concurrent processes using to prepare data\n",
    "    device: str = 'cuda'  # device to use for training.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "def prediction(model, device, batch_input, max_prob=True):\n",
    "    \"\"\"\n",
    "    get prediction for batch inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    # send model to cpu/cuda according to your system configuration\n",
    "    model.to(device)\n",
    "    \n",
    "    # it is important to do model.eval() before prediction\n",
    "    model.eval()\n",
    "\n",
    "    data = batch_input.to(device)\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # Score to probability using softmax\n",
    "    prob = F.softmax(output, dim=1)\n",
    "    \n",
    "    if max_prob:\n",
    "        # get the max probability\n",
    "        pred_prob = prob.data.max(dim=1)[0]\n",
    "    else:\n",
    "        pred_prob = prob.data\n",
    "    \n",
    "    # get the index of the max probability\n",
    "    pred_index = prob.data.max(dim=1)[1]\n",
    "    \n",
    "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()\n",
    "\n",
    "def get_target_and_prob(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    get targets and prediction probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    pred_prob = []\n",
    "    targets = []\n",
    "    \n",
    "    for _, (data, target) in enumerate(dataloader):\n",
    "        \n",
    "        _, prob = prediction(model, device, data, max_prob=False)\n",
    "        \n",
    "        pred_prob.append(prob)\n",
    "        \n",
    "        target = target.numpy()\n",
    "        targets.append(target)\n",
    "        \n",
    "    targets = np.concatenate(targets)\n",
    "    targets = targets.astype(int)\n",
    "    pred_prob = np.concatenate(pred_prob, axis=0)\n",
    "    \n",
    "    return targets, pred_prob\n",
    "\n",
    "def get_random_inputs_labels(inputs, targets, n=100):\n",
    "    \"\"\"\n",
    "    get random inputs and labels\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(inputs) == len(targets)\n",
    "\n",
    "    rand_indices = torch.randperm(len(targets))\n",
    "    \n",
    "    data = inputs[rand_indices][:n]\n",
    "    \n",
    "    labels = targets[rand_indices][:n]\n",
    "    \n",
    "    class_labels = [classesArray[lab] for lab in labels]\n",
    "    \n",
    "    return data, class_labels\n",
    "\n",
    "def add_data_embeddings(dataset, tb_writer, n=100):\n",
    "    \"\"\"\n",
    "    Add a few inputs and labels to tensorboard. \n",
    "    \"\"\"\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=n, num_workers=4, shuffle=True)\n",
    "    \n",
    "    images, labels = next(iter(dataloader))\n",
    "    \n",
    "    tb_writer.add_embedding(mat = images.view(-1, 3 * 224 * 224), \n",
    "                            metadata=labels, \n",
    "                            label_img=images)\n",
    "    \n",
    "    return\n",
    "\n",
    "def add_pr_curves_to_tensorboard(model, dataloader, device, tb_writer, epoch, num_classes=10):\n",
    "    \"\"\"\n",
    "    Add precession and recall curve to tensorboard.\n",
    "    \"\"\"\n",
    "    \n",
    "    targets, pred_prob = get_target_and_prob(model, dataloader, device)\n",
    "    \n",
    "    for cls_idx in range(num_classes):\n",
    "        binary_target = targets == cls_idx\n",
    "        true_prediction_prob = pred_prob[:, cls_idx]\n",
    "        \n",
    "        tb_writer.add_pr_curve(classesArray[cls_idx], \n",
    "                               binary_target, \n",
    "                               true_prediction_prob, \n",
    "                               global_step=epoch)\n",
    "        \n",
    "    return\n",
    "\n",
    "def add_wrong_prediction_to_tensorboard(data_root, model, dataloader, device, tb_writer, \n",
    "                                        epoch, tag='Wrong_Predections', max_images='all'):\n",
    "    \"\"\"\n",
    "    Add wrong predicted images to tensorboard.\n",
    "    \"\"\"\n",
    "    #number of images in one row\n",
    "    num_images_per_row = 8\n",
    "    im_scale = 3\n",
    "    \n",
    "    plot_images = []\n",
    "    wrong_labels = []\n",
    "    pred_prob = []\n",
    "    right_label = []\n",
    "    \n",
    "    mean, std = get_mean_std(data_root)\n",
    "    \n",
    "    for _, (data, target) in enumerate(dataloader):\n",
    "        \n",
    "        \n",
    "        images = data.numpy()\n",
    "        pred, prob = prediction(model, device, data)\n",
    "        target = target.numpy()\n",
    "        indices = pred.astype(int) != target.astype(int)\n",
    "        \n",
    "        plot_images.append(images[indices])\n",
    "        wrong_labels.append(pred[indices])\n",
    "        pred_prob.append(prob[indices])\n",
    "        right_label.append(target[indices])\n",
    "        \n",
    "    plot_images = np.concatenate(plot_images, axis=0).squeeze()\n",
    "    plot_images = (np.moveaxis(plot_images, 1, -1) * std) + mean\n",
    "    print('plot_images.shape: {}'.format(plot_images.shape))\n",
    "    print(plot_images.min())\n",
    "    print(plot_images.max())\n",
    "    wrong_labels = np.concatenate(wrong_labels)\n",
    "    wrong_labels = wrong_labels.astype(int)\n",
    "    right_label = np.concatenate(right_label)\n",
    "    right_label = right_label.astype(int)\n",
    "    pred_prob = np.concatenate(pred_prob)\n",
    "    \n",
    "    \n",
    "    if max_images == 'all':\n",
    "        num_images = len(images)\n",
    "    else:\n",
    "        num_images = min(len(plot_images), max_images)\n",
    "        \n",
    "    fig_width = num_images_per_row * im_scale\n",
    "    \n",
    "    if num_images % num_images_per_row == 0:\n",
    "        num_row = num_images/num_images_per_row\n",
    "    else:\n",
    "        num_row = int(num_images/num_images_per_row) + 1\n",
    "        \n",
    "    fig_height = num_row * im_scale\n",
    "        \n",
    "    plt.style.use('default')\n",
    "    plt.rcParams[\"figure.figsize\"] = (fig_width, fig_height)\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.subplot(num_row, num_images_per_row, i+1, xticks=[], yticks=[])\n",
    "        plt.imshow(plot_images[i].astype('uint8'))\n",
    "        plt.gca().set_title('{0}({1:.2}), {2}'.format(animal_classes[wrong_labels[i]], \n",
    "                                                          pred_prob[i], \n",
    "                                                          animal_classes[right_label[i]]))\n",
    "        \n",
    "    tb_writer.add_figure(tag, fig, global_step=epoch)\n",
    "    \n",
    "    return\n",
    "\n",
    "def add_pr_curves_to_tensorboard(model, dataloader, device, tb_writer, epoch, num_classes=3):\n",
    "    \"\"\"\n",
    "    Add precession and recall curve to tensorboard.\n",
    "    \"\"\"\n",
    "    \n",
    "    targets, pred_prob = get_target_and_prob(model, dataloader, device)\n",
    "    \n",
    "    for cls_idx in range(num_classes):\n",
    "        binary_target = targets == cls_idx\n",
    "        true_prediction_prob = pred_prob[:, cls_idx]\n",
    "        \n",
    "        tb_writer.add_pr_curve(classesArray[cls_idx], \n",
    "                               binary_target, \n",
    "                               true_prediction_prob, \n",
    "                               global_step=epoch)\n",
    "        \n",
    "    return\n",
    "\n",
    "def add_model_weights_as_histogram(model, tb_writer, epoch):\n",
    "    for name, param in model.named_parameters():\n",
    "        tb_writer.add_histogram(name.replace('.', '/'), param.data.cpu().abs(), epoch)\n",
    "    return\n",
    "\n",
    "def add_network_graph_tensorboard(model, inputs, tb_writer):\n",
    "    tb_writer.add_graph(model, inputs)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
    "\n",
    "Define methods or classes that will be used in model evaluation—for example, accuracy, f1-score, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(train_loss, val_loss, train_acc, val_acc, colors, \n",
    "                       loss_legend_loc='upper center', acc_legend_loc='upper left', \n",
    "                       fig_size=(20, 10), sub_plot1=(1, 2, 1), sub_plot2=(1, 2, 2)):\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n",
    "    \n",
    "    for i in range(len(train_loss)):\n",
    "        x_train = range(len(train_loss[i]))\n",
    "        x_val = range(len(val_loss[i]))\n",
    "        \n",
    "        min_train_loss = train_loss[i].min()\n",
    "        \n",
    "        min_val_loss = val_loss[i].min()\n",
    "        \n",
    "        plt.plot(x_train, train_loss[i], linestyle='-', color='tab:{}'.format(colors[i]), \n",
    "                 label=\"TRAIN LOSS ({0:.4})\".format(min_train_loss))\n",
    "        plt.plot(x_val, val_loss[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n",
    "                 label=\"VALID LOSS ({0:.4})\".format(min_val_loss))\n",
    "        \n",
    "    plt.xlabel('epoch no.')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc=loss_legend_loc)\n",
    "    plt.title('Training and Validation Loss')\n",
    "        \n",
    "    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n",
    "    \n",
    "    for i in range(len(train_acc)):\n",
    "        x_train = range(len(train_acc[i]))\n",
    "        x_val = range(len(val_acc[i]))\n",
    "        \n",
    "        max_train_acc = train_acc[i].max() \n",
    "        \n",
    "        max_val_acc = val_acc[i].max() \n",
    "        \n",
    "        plt.plot(x_train, train_acc[i], linestyle='-', color='tab:{}'.format(colors[i]), \n",
    "                 label=\"TRAIN ACC ({0:.4})\".format(max_train_acc))\n",
    "        plt.plot(x_val, val_acc[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n",
    "                 label=\"VALID ACC ({0:.4})\".format(max_val_acc))\n",
    "        \n",
    "    plt.xlabel('epoch no.')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=acc_legend_loc)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    \n",
    "    fig.savefig('sample_loss_acc_plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, device, model_dir='../models', model_file_name='food_classifier.pt'):\n",
    "    \n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    model_path = os.path.join(model_dir, model_file_name)\n",
    "\n",
    "    # make sure you transfer the model to cpu.\n",
    "    if device == 'cuda':\n",
    "        model.to('cpu')\n",
    "\n",
    "    # save the state_dict\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        model.to('cuda')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_dir='../models', model_file_name='food_classifier.pt'):\n",
    "    model_path = os.path.join(model_dir, model_file_name)\n",
    "\n",
    "    # loading the model and getting model parameters by using load_state_dict\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
    "\n",
    "Write the methods or classes that will be used for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    for data, target in test_loader:\n",
    "        indx_target = target.clone()\n",
    "        data = data.to(train_config.device)\n",
    "        \n",
    "        target = target.to(train_config.device)\n",
    "        \n",
    "        output = model(data)\n",
    "        # add loss for each mini batch\n",
    "        test_loss += F.cross_entropy(output, target).item()\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1] \n",
    "        \n",
    "        # add correct prediction count\n",
    "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "    # average over number of mini-batches\n",
    "    test_loss = test_loss / len(test_loader)  \n",
    "    \n",
    "    # average over number of dataset\n",
    "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "    \n",
    "    print(\n",
    "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int, tb_writer: SummaryWriter\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mood\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "       \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gardients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "        \n",
    "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:\n",
    "            \n",
    "            total_batch = epoch_idx * len(train_loader.dataset)/train_config.batch_size + batch_idx\n",
    "            tb_writer.add_scalar('Loss/train-batch', loss.item(), total_batch)\n",
    "            tb_writer.add_scalar('Accuracy/train-batch', acc, total_batch)\n",
    "    \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    print('Epoch: {} \\nTrain Loss: {:.6f} Acc: {:.4f}'.format(epoch_idx, epoch_loss, epoch_acc))\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
    "\n",
    "Define your model in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 256x256x3\n",
    "classes = 13\n",
    "nodes = 128\n",
    "k = 1\n",
    "\n",
    "#nn.BatchNorm2d(64),\n",
    "\n",
    "\n",
    "def pretrained_resnet18(transfer_learning=True, num_class=classes):\n",
    "    resnet = torchvision.models.resnet18(pretrained=True)\n",
    "    \n",
    "    if transfer_learning:\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    last_layer_in = resnet.fc.in_features\n",
    "    resnet.fc = nn.Linear(last_layer_in, num_class)\n",
    "    \n",
    "    return resnet\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16 * k, kernel_size=3),\n",
    "            nn.BatchNorm2d(16 * k),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16 * k, out_channels=32 * k, kernel_size=3),\n",
    "            nn.BatchNorm2d(32 * k),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32 * k, out_channels=64 * k, kernel_size=3),\n",
    "            nn.BatchNorm2d(64 * k),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64 * k, out_channels=128 * k, kernel_size=3),\n",
    "            nn.BatchNorm2d(128 * k),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=128 * k, out_channels=nodes * k, kernel_size=3),\n",
    "            nn.BatchNorm2d(nodes * k),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            nn.Linear(in_features=3200 * k, out_features=10), \n",
    "            nn.Dropout(0.5), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=10, out_features=classes)\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_dir='../models', model_file_name='food_classifier.pt'):\n",
    "    model_path = os.path.join(model_dir, model_file_name)\n",
    "\n",
    "    # loading the model and getting model parameters by using load_state_dict\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, optimizer, scheduler=None, system_configuration=SystemConfiguration(), \n",
    "         training_configuration=TrainingConfiguration(), data_augmentation=True):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # Tensorboard writer for visualization\n",
    "    tb_writer = SummaryWriter('../logs/kenyan_food_log')\n",
    "    \n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lowers batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 4\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        tb_writer=tb_writer,\n",
    "        num_workers=num_workers_to_set,\n",
    "        data_augmentation=data_augmentation\n",
    "    )\n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    add_wrong_prediction_to_tensorboard(training_configuration.data_root,\n",
    "                                        model,\n",
    "                                        test_loader, \n",
    "                                        training_configuration.device, \n",
    "                                        tb_writer,\n",
    "                                        0,\n",
    "                                        max_images=300)\n",
    "    \n",
    "    # trainig time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        # Calculate Initial Test Loss\n",
    "        init_val_loss, init_val_accuracy = validate(training_configuration, model, test_loader)\n",
    "        print(\"Initial Test Loss : {:.6f}, \\nInitial Test Accuracy : {:.3f}%\\n\".format(init_val_loss, init_val_accuracy*100))\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch, tb_writer)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "        \n",
    "        # add scalar (loss/accuracy) to tensorboard\n",
    "        tb_writer.add_scalar('Loss/Train',train_loss, epoch)\n",
    "        tb_writer.add_scalar('Accuracy/Train', train_acc, epoch)\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        # add time metadata to tensorboard\n",
    "        tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch)\n",
    "        tb_writer.add_scalar('Time/speed_epoch', speed_epoch, epoch)\n",
    "        tb_writer.add_scalar('Time/speed_batch', speed_batch, epoch)\n",
    "        tb_writer.add_scalar('Time/eta', eta, epoch)\n",
    "        \n",
    "        print(\n",
    "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "                elapsed_time, speed_epoch, speed_batch, eta\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            # add scalar (loss/accuracy) to tensorboard\n",
    "            tb_writer.add_scalar('Loss/Validation', current_loss, epoch)\n",
    "            tb_writer.add_scalar('Accuracy/Validation', current_accuracy, epoch)\n",
    "            \n",
    "            # add scalars (loss/accuracy) to tensorboard\n",
    "            tb_writer.add_scalars('Loss/train-val', {'train': train_loss, \n",
    "                                           'validation': current_loss}, epoch)\n",
    "            tb_writer.add_scalars('Accuracy/train-val', {'train': train_acc, \n",
    "                                               'validation': current_accuracy}, epoch)\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                print('Model Improved. Saving the Model...\\n')\n",
    "                save_model(model, device=training_configuration.device)\n",
    "                \n",
    "            # add wrong predicted image to tensorboard\n",
    "            add_wrong_prediction_to_tensorboard(training_configuration.data_root,\n",
    "                                                model,\n",
    "                                                test_loader, \n",
    "                                                training_configuration.device, \n",
    "                                                tb_writer,\n",
    "                                                epoch,\n",
    "                                                max_images=300)\n",
    "        \n",
    "        # Decay learning rate\n",
    "        if scheduler is not None:\n",
    "            scheduler.step() # scheduler step/ update learning rate\n",
    "            print('Stepping scheduler this epoch. ', 'LR:', scheduler.get_lr())\n",
    " \n",
    "        add_model_weights_as_histogram(model, tb_writer, epoch)\n",
    "    \n",
    "        # add pr curves to tensor board\n",
    "        add_pr_curves_to_tensorboard(model, test_loader, \n",
    "                                     training_configuration.device, \n",
    "                                     tb_writer, epoch, num_classes=3)\n",
    "        \n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    tb_writer.close()\n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
    "\n",
    "Define your methods or classes which are not covered in the above sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess_transforms():\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_common_transforms(mean=(0.4611, 0.4359, 0.3905), std=(0.2193, 0.2150, 0.2109)):\n",
    "    preprocess = image_preprocess_transforms()\n",
    "    \n",
    "    common_transforms = transforms.Compose([\n",
    "        preprocess,\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    return common_transforms\n",
    "\n",
    "def data_augmentation_preprocess(mean=(0.4611, 0.4359, 0.3905), std=(0.2193, 0.2150, 0.2109)):\n",
    "    preprocess = image_preprocess_transforms()\n",
    "    \n",
    "    data_augmentation_transforms = transforms.Compose([\n",
    "        torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomRotation(20),\n",
    "        preprocess,\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    return data_augmentation_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(data_root, num_workers=4):\n",
    "    \n",
    "    transform = image_preprocess_transforms()\n",
    "    training_configuration = TrainingConfiguration()\n",
    "    \n",
    "    file_path = os.path.join(data_root, 'images')\n",
    "    \n",
    "    dataset = datasets.ImageFolder(root=file_path, transform=transform)\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=training_configuration.batch_size,\n",
    "                                         num_workers=num_workers,\n",
    "                                         shuffle=True)\n",
    "\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    \n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "\n",
    "    mean /= len(loader.dataset)\n",
    "    std /= len(loader.dataset)\n",
    "    \n",
    "    print('mean: {}, std: {}'.format(mean, std))\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
    "\n",
    "Choose your optimizer and LR-scheduler and use the above methods and classes to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#model = MyModel()\n",
    "#model = torchvision.models.resnet50()\n",
    "model = pretrained_resnet18(transfer_learning=True)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# get optimizer\n",
    "train_config = TrainingConfiguration()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=train_config.init_learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "# optimizer\n",
    "#optimizer = optim.Adam(\n",
    "#    model.parameters(),\n",
    "#    lr = train_config.init_learning_rate\n",
    "#)\n",
    "\n",
    "#scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "decayRate = 0.96\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decayRate)\n",
    "\n",
    "#optimizer = optim.SGD(\n",
    "#    model.parameters(),\n",
    "#    lr=train_config.init_learning_rate\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate\n",
    "model, train_loss, train_acc, val_loss, val_acc = main(model, optimizer, scheduler=scheduler, data_augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(train_loss=[train_loss], \n",
    "                   val_loss=[val_loss], \n",
    "                   train_acc=[train_acc], \n",
    "                   val_acc=[val_acc], \n",
    "                   colors=['blue'], \n",
    "                   loss_legend_loc='upper center', \n",
    "                   acc_legend_loc='upper left')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "train_loader, test_loader = get_data(\n",
    "        batch_size=train_config.batch_size,\n",
    "        data_root=train_config.data_root,\n",
    "        num_workers=train_config.num_workers,\n",
    "        data_augmentation=True\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "\n",
    "truelabels = []\n",
    "predictions = []\n",
    "\n",
    "classes = classesArray\n",
    "\n",
    "for data, target in test_loader:\n",
    "    for label in target.cpu().data.numpy():\n",
    "        truelabels.append(label)\n",
    "    for prediction in model.cpu()(data).data.numpy().argmax(1):\n",
    "        predictions.append(prediction) \n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(truelabels, predictions)\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. TensorBoard Dev Scalars Log Link [5 Points]</font>\n",
    "\n",
    "Share your tensorboard scalars logs link in this section. You can also share (not mandatory) your GitHub link if you have pushed this project in GitHub. \n",
    "\n",
    "For example, [Find Project2 logs here](https://tensorboard.dev/experiment/kMJ4YU0wSNG0IkjrluQ5Dg/#scalars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/alanhaugen/pytorch-opencv-course-classification\n",
    "\n",
    "[Find Project2 logs here](https://tensorboard.dev/experiment/IKzTP79DTNaUYpWi3wlsbg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
    "\n",
    "Share your Kaggle profile link here with us so that we can give points for the competition score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/alanhaugen"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
